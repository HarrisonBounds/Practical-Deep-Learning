{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuT9OAeGlwjx"
      },
      "source": [
        "# Python Basics with NumPy\n",
        "\n",
        "Welcome to your first assignment. This assignment gives you a brief introduction to NumPy library in Python. Please complete following exercises to collect your credits.\n",
        "1. Exercise 1.1.1 (5%)\n",
        "2. Exercise 1.1.2 (5%)\n",
        "3. Exercise 1.2 (10%)\n",
        "4. Exercise 1.3 (10%)\n",
        "5. Exercise 1.4 (10%)\n",
        "6. Exercise 2.1 (40%)\n",
        "7. Exercise 2.2 (20%)\n",
        "\n",
        "\n",
        "**Instructions:**\n",
        "- Avoid using for-loops and while-loops, unless you are explicitly told to do so.\n",
        "- Write your code between the ### START CODE HERE ### and ### END CODE HERE ### comments. **Do not modify code out of the designated area.**\n",
        "- After coding your function, run the cell to check if your result is correct.\n",
        "\n",
        "**After this assignment you will:**\n",
        "- Be able to use iPython Notebooks\n",
        "- Be able to use numpy functions and numpy matrix/vector operations\n",
        "- Understand the concept of \"broadcasting\"\n",
        "- Be able to vectorize code\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIe7FyMilwj5"
      },
      "source": [
        "## 1 - NumPy Functions ##\n",
        "\n",
        "[NumPy](https://numpy.org/) is an open source project aiming to enable numerical computing with Python.  It is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation and much more. \n",
        "\n",
        "In this exercise you will learn how to construct customized functions using NumPy. \n",
        "\n",
        "### 1.1 - Sigmoid function ###\n",
        "$\\sigma(x) = \\frac{1}{1+e^{-x}}$ is also known as the logistic function. It is usually used as an non-linear activation in deep learning.\n",
        "\n",
        "![](https://mathworld.wolfram.com/images/eps-svg/SigmoidFunction_701.svg)\n",
        "\n",
        "#### **(5%) Exercise 1.1.1: Build a sigmoid function using `math` library**\n",
        "The function returns the sigmoid of a real number x. Use **`math.exp(x)`** for the exponential function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "w3f8uk8llwj6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9525741268224334\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "def math_sigmoid(x):\n",
        "    \"\"\"\n",
        "    Compute sigmoid of x.\n",
        "\n",
        "    Arguments:\n",
        "    x -- A scalar\n",
        "\n",
        "    Return:\n",
        "    s -- sigmoid(x)\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ### (≈ 1 line of code)\n",
        "    s = 1 / (1 + math.exp(-x))\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return s\n",
        "\n",
        "y = math_sigmoid(3)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6nAbKh-lwj7"
      },
      "source": [
        "> Expected Output: \n",
        "```python\n",
        "0.9525741268224334\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SX3OZTITNMbO"
      },
      "source": [
        "Actually, we rarely use `math` library in deep learning because we mostly use matrices and vectors, instead of real numbers. This is why numpy is more useful. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTFsqRuqlwj7"
      },
      "outputs": [],
      "source": [
        "# You'll get an error since math library cannot deal with vectors\n",
        "x = [1, 2, 3]\n",
        "math_sigmoid(x) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGaI8PJfP_XK"
      },
      "source": [
        "By using numpy, x could now be either a real number, a vector, or a matrix. The data structures we use in numpy to represent these shapes (vectors, matrices...) are called numpy arrays. If $ x = (x_1, x_2, ..., x_n)$ then $np.exp(x)$ will apply the exponential function to every element of x. The output will thus be: $np.exp(x) = (e^{x_1}, e^{x_2}, ..., e^{x_n})$. Also, $x + 3$ or $\\frac{1}{x}$ will perform elementwise operations. And the output should be a vector with the same size as x."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVCb_7wElwj8",
        "outputId": "e649364a-f8ec-41ec-8a91-df046d644acd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 2.71828183  7.3890561  20.08553692]\n",
            "[4 5 6]\n",
            "[1.         0.5        0.33333333]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.array([1, 2, 3])\n",
        "print(np.exp(x))\n",
        "print(x + 3)\n",
        "print(1 / x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOswWkZ0lwj9"
      },
      "source": [
        "\n",
        "#### **(5%) Exercise 1.1.2: Implement the sigmoid function using numpy.** \n",
        "The function returns the sigmoid of either a real number, or a vector, or a matrix. \n",
        "$$ \\text{For } x \\in \\mathbb{R}^n \\text{,     } \\sigma(x) = \\sigma\\begin{pmatrix}\n",
        "    x_1  \\\\\n",
        "    x_2  \\\\\n",
        "    ...  \\\\\n",
        "    x_n  \\\\\n",
        "\\end{pmatrix} = \\begin{pmatrix}\n",
        "    \\frac{1}{1+e^{-x_1}}  \\\\\n",
        "    \\frac{1}{1+e^{-x_2}}  \\\\\n",
        "    ...  \\\\\n",
        "    \\frac{1}{1+e^{-x_n}}  \\\\\n",
        "\\end{pmatrix}\\tag{1} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FhBPccRLlwj9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.73105858, 0.88079708, 0.95257413])"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np \n",
        "\n",
        "def numpy_sigmoid(x):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid of x\n",
        "\n",
        "    Arguments:\n",
        "    x -- A scalar or numpy array of any size\n",
        "\n",
        "    Return:\n",
        "    s -- sigmoid(x)\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ### (≈ 1 line of code)\n",
        "    s =  1 / (1 + np.exp(-x))\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return s\n",
        "\n",
        "x = np.array([1, 2, 3])\n",
        "numpy_sigmoid(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nou6SkKDlwj-"
      },
      "source": [
        "> Expected Output:\n",
        "```python\n",
        "array([0.73105858, 0.88079708, 0.95257413])\n",
        "``` \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezNagT8elwj-"
      },
      "source": [
        "### 1.2 - Sigmoid gradient\n",
        "\n",
        "Gradient is the essential tool to perform backpropagation in deep learning. You will need to compute gradients to optimize loss functions.\n",
        "\n",
        "#### **(10%) 1.2 Exercise: Compute the gradient of the sigmoid function**\n",
        "The formula is: $$\\sigma'(x) = \\sigma(x) (1 - \\sigma(x))\\tag{2}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4JYGfsyklwj-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "grad(x) = [0.19661193 0.10499359 0.04517666]\n"
          ]
        }
      ],
      "source": [
        "def grad_sigmoid(x):\n",
        "    \"\"\"\n",
        "    Compute the gradient (also called the slope or derivative) of the sigmoid function with respect to its input x.\n",
        "    You can store the output of the sigmoid function into variables and then use it to calculate the gradient.\n",
        "    \n",
        "    Arguments:\n",
        "    x -- A scalar or numpy array\n",
        "\n",
        "    Return:\n",
        "    ds -- Your computed gradient.\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ### (≈ 2 lines of code)\n",
        "    s = 1 / (1 + np.exp(-x))\n",
        "    ds = s * (1 - s)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return ds\n",
        "\n",
        "x = np.array([1, 2, 3])\n",
        "print (f\"grad(x) = {grad_sigmoid(x)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBnZAuZ-lwj-"
      },
      "source": [
        "> Expected Output: \n",
        "```python\n",
        "grad(x) = [0.19661193 0.10499359 0.04517666]\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDmC5kojlwj_"
      },
      "source": [
        "### 1.3 - Reshaping arrays ###\n",
        "\n",
        "Two common numpy functions used in deep learning are [np.shape](https://numpy.org/doc/stable/reference/generated/numpy.shape.html) and [np.reshape()](https://numpy.org/doc/stable/reference/generated/numpy.reshape.html). \n",
        "- X.shape is used to get the shape (dimension) of a matrix/vector X. \n",
        "- X.reshape(...) is used to reshape X into some other dimension. \n",
        "\n",
        "For example, in computer vision, a colored image is usually represented by a 3-dimensional array with shape $(width, height, 3)$. You can convert it into a vector, or 1-dimensional array with shape $(width*height*3, 1)$.\n",
        "\n",
        "![](https://eli.thegreenplace.net/images/2015/row-major-3D.png)\n",
        "\n",
        "#### **(10%) Exercise: 1.3 Build a function to convert an image into a vector**\n",
        "The function takes an input of shape (width, height, 3) and returns a vector of shape (width\\*height\\*3, 1). \n",
        "**Please don't hardcode the dimensions of image as a constant. Instead, look up the quantities you need with `image.shape[0]`, etc.** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "collapsed": true,
        "id": "RvtZ5nfmlwj_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vectorized image = [[0.67826139]\n",
            " [0.29380381]\n",
            " [0.90714982]\n",
            " [0.52835647]\n",
            " [0.4215251 ]\n",
            " [0.45017551]\n",
            " [0.92814219]\n",
            " [0.96677647]\n",
            " [0.85304703]\n",
            " [0.52351845]\n",
            " [0.19981397]\n",
            " [0.27417313]\n",
            " [0.60659855]\n",
            " [0.00533165]\n",
            " [0.10820313]\n",
            " [0.49978937]\n",
            " [0.34144279]\n",
            " [0.94630077]]\n"
          ]
        }
      ],
      "source": [
        "def image2vector(image):\n",
        "    \"\"\"\n",
        "    Argument:\n",
        "    image -- a numpy array of shape (length, height, depth)\n",
        "    \n",
        "    Returns:\n",
        "    v -- a vector of shape (length*height*depth, 1)\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ### (≈ 1 line of code)\n",
        "    v = image.reshape(image.shape[0] * image.shape[1] * image.shape[2], 1)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return v\n",
        "\n",
        "image = np.array([[[ 0.67826139,  0.29380381],\n",
        "        [ 0.90714982,  0.52835647],\n",
        "        [ 0.4215251 ,  0.45017551]],\n",
        "\n",
        "       [[ 0.92814219,  0.96677647],\n",
        "        [ 0.85304703,  0.52351845],\n",
        "        [ 0.19981397,  0.27417313]],\n",
        "\n",
        "       [[ 0.60659855,  0.00533165],\n",
        "        [ 0.10820313,  0.49978937],\n",
        "        [ 0.34144279,  0.94630077]]])\n",
        "\n",
        "print (f\"vectorized image = {image2vector(image)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LvRhmo3lwj_"
      },
      "source": [
        ">Expected Output: \n",
        "```python\n",
        "vectorized image = [[0.67826139]\n",
        " [0.29380381]\n",
        " [0.90714982]\n",
        " [0.52835647]\n",
        " [0.4215251 ]\n",
        " [0.45017551]\n",
        " [0.92814219]\n",
        " [0.96677647]\n",
        " [0.85304703]\n",
        " [0.52351845]\n",
        " [0.19981397]\n",
        " [0.27417313]\n",
        " [0.60659855]\n",
        " [0.00533165]\n",
        " [0.10820313]\n",
        " [0.49978937]\n",
        " [0.34144279]\n",
        " [0.94630077]]\n",
        " ```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atdm5mEBlwj_"
      },
      "source": [
        "### 1.4 - Normalization\n",
        "\n",
        "A common technique used in Deep Learning is normalization. The L-2 norm of a vector can be calculated as $\\|x\\|_2=\\sqrt{\\Sigma_{i}{x_i}^2}$. Normalization means changing a vector $x$ to $\\frac{x}{\\| x\\|}$ (dividing each row of x by its norm).\n",
        "\n",
        "For example, if \n",
        "\n",
        "$$x = \n",
        "\\begin{bmatrix}\n",
        "    0 & 3 & 4 \\\\\n",
        "    2 & 6 & 4 \\\\\n",
        "\\end{bmatrix}\\tag{3}$$ \n",
        "\n",
        "then \n",
        "$$\\| x\\| = \\begin{bmatrix}\n",
        "    5 \\\\\n",
        "    \\sqrt{56} \\\\\n",
        "\\end{bmatrix}\\tag{4} $$\n",
        "\n",
        "and        \n",
        "$$ normalized\\_x = \\frac{x}{\\| x\\|} = \\begin{bmatrix}\n",
        "    0 & \\frac{3}{5} & \\frac{4}{5} \\\\\n",
        "    \\frac{2}{\\sqrt{56}} & \\frac{6}{\\sqrt{56}} & \\frac{4}{\\sqrt{56}} \\\\\n",
        "\\end{bmatrix}\\tag{5}$$ \n",
        "\n",
        "You can use `np.linalg.norm(x, axis = 1, keepdims = True)` to calculate norm of row vectors in $x$.\n",
        "\n",
        "Although $x$ is a 2-d array and $\\|x\\|$ is a 1-d arrary, the operation in (4) can be easily performed. This is called broadcasting. For the full details on broadcasting, you can read the official [broadcasting documentation](https://numpy.org/doc/stable/user/basics.broadcasting.html).\n",
        "\n",
        "\n",
        "#### **(10%) Exercise 1.4: Implement row normalization.** \n",
        "After applying this function to an input matrix x, each row of x should be a unit vector (length=1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": true,
        "id": "fDXiCVudlwkA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "normalizeRows(x) = [[0.         0.6        0.8       ]\n",
            " [0.13736056 0.82416338 0.54944226]]\n"
          ]
        }
      ],
      "source": [
        "def normalizeRows(x):\n",
        "    \"\"\"\n",
        "    Implement a function that normalizes each row of the matrix x (to have unit length).\n",
        "    \n",
        "    Argument:\n",
        "    x -- A numpy matrix of shape (n, m)\n",
        "    \n",
        "    Returns:\n",
        "    x -- The normalized (by row) numpy matrix. You are allowed to modify x.\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ### (≈ 2 lines of code)\n",
        "    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)    \n",
        "    y = np.linalg.norm(x, axis=1, keepdims=True)\n",
        "    x = x / y\n",
        "    # Divide x by its norm.\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return x\n",
        "\n",
        "x = np.array([\n",
        "    [0, 3, 4],\n",
        "    [1, 6, 4]])\n",
        "print(f\"normalizeRows(x) = {normalizeRows(x)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qecUM8bjlwkA"
      },
      "source": [
        ">Expected Output: \n",
        "```python\n",
        "normalizeRows(x) = [[0.         0.6        0.8       ]\n",
        " [0.13736056 0.82416338 0.54944226]]\n",
        " ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7jK4S--lwkC"
      },
      "source": [
        "## 2. Vectorization\n",
        "\n",
        "### 2.1 NumPy optimized vector operations\n",
        "In deep learning, you deal with very large datasets. Hence, a non-computationally-optimal function can become a huge bottleneck in your algorithm and can result in a model that takes ages to run. To make sure that your code is  computationally efficient, you will use vectorization. For example, try to tell the difference between the following implementations of the dot/outer/elementwise product."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMz3ORColwkC",
        "outputId": "c850b47f-03ea-4f5b-ec72-ce2e80edc453"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dot = 278\n",
            "----- Computation time = 0.1961709999998007 ms\n",
            "\n",
            "outer = [[81. 18. 18. 81.  0. 81. 18. 45.  0.  0. 81. 18. 45.  0.  0.]\n",
            " [18.  4.  4. 18.  0. 18.  4. 10.  0.  0. 18.  4. 10.  0.  0.]\n",
            " [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [63. 14. 14. 63.  0. 63. 14. 35.  0.  0. 63. 14. 35.  0.  0.]\n",
            " [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [81. 18. 18. 81.  0. 81. 18. 45.  0.  0. 81. 18. 45.  0.  0.]\n",
            " [18.  4.  4. 18.  0. 18.  4. 10.  0.  0. 18.  4. 10.  0.  0.]\n",
            " [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
            "----- Computation time = 0.33998999999962365 ms\n",
            "\n",
            "elementwise multiplication = [81.  4. 10.  0.  0. 63. 10.  0.  0.  0. 81.  4. 25.  0.  0.]\n",
            "----- Computation time = 0.12698000000010978 ms\n",
            "\n",
            "gdot = [26.1505803  16.37561645 22.03705494]\n",
            "----- Computation time = 0.12343099999956308 ms\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\n",
        "x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n",
        "\n",
        "### CLASSIC DOT PRODUCT OF VECTORS IMPLEMENTATION ###\n",
        "tic = time.process_time()\n",
        "dot = 0\n",
        "for i in range(len(x1)):\n",
        "    dot+= x1[i]*x2[i]\n",
        "toc = time.process_time()\n",
        "print(f\"dot = {dot}\\n----- Computation time = {1000*(toc - tic)} ms\\n\")\n",
        "\n",
        "### CLASSIC OUTER PRODUCT IMPLEMENTATION ###\n",
        "tic = time.process_time()\n",
        "outer = np.zeros((len(x1),len(x2))) # we create a len(x1)-by-len(x2) matrix with only zeros\n",
        "for i in range(len(x1)):\n",
        "    for j in range(len(x2)):\n",
        "        outer[i,j] = x1[i]*x2[j]\n",
        "toc = time.process_time()\n",
        "print(f\"outer = {outer}\\n----- Computation time = {1000*(toc - tic)} ms\\n\")\n",
        "\n",
        "### CLASSIC ELEMENTWISE IMPLEMENTATION ###\n",
        "tic = time.process_time()\n",
        "mul = np.zeros(len(x1))\n",
        "for i in range(len(x1)):\n",
        "    mul[i] = x1[i]*x2[i]\n",
        "toc = time.process_time()\n",
        "print(f\"elementwise multiplication = {mul}\\n----- Computation time = {1000*(toc - tic)} ms\\n\")\n",
        "\n",
        "### CLASSIC GENERAL DOT PRODUCT IMPLEMENTATION ###\n",
        "W = np.random.rand(3,len(x1)) # Random 3*len(x1) numpy array\n",
        "tic = time.process_time()\n",
        "gdot = np.zeros(W.shape[0])\n",
        "for i in range(W.shape[0]):\n",
        "    for j in range(len(x1)):\n",
        "        gdot[i] += W[i,j]*x1[j]\n",
        "toc = time.process_time()\n",
        "print(f\"gdot = {gdot}\\n----- Computation time = {1000*(toc - tic)} ms\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksLjxQc0nUFh"
      },
      "source": [
        "#### **(40%) Exercise 2.1 Optimize vector operations using NumPy functions**\n",
        "Find corresponding NumPy functions for above operations (dot product, outer product, elementwise multiplication). Use NumPy functions to re-calculate these operations and compute the time comsumptions. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "U8DZAUeVlwkD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dot = 278\n",
            "----- Computation time = 0.0 ms\n",
            "\n",
            "outer = [[81 18 18 81  0 81 18 45  0  0 81 18 45  0  0]\n",
            " [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0]\n",
            " [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [63 14 14 63  0 63 14 35  0  0 63 14 35  0  0]\n",
            " [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [81 18 18 81  0 81 18 45  0  0 81 18 45  0  0]\n",
            " [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0]\n",
            " [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
            "----- Computation time = 0.0 ms\n",
            "\n",
            "elementwise multiplication = [81  4 10  0  0 63 10  0  0  0 81  4 25  0  0]\n",
            "----- Computation time = 0.0 ms\n",
            "\n",
            "gdot = [23.20870724 20.28636183 23.05462684]\n",
            "----- Computation time = 0.0 ms\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\n",
        "x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n",
        "\n",
        "### VECTORIZED DOT PRODUCT OF VECTORS ###\n",
        "tic = time.process_time()\n",
        "### START CODE HERE ###\n",
        "dot = np.dot(x1, x2, out=None)\n",
        "### END CODE HERE ###\n",
        "toc = time.process_time()\n",
        "print(f\"dot = {dot}\\n----- Computation time = {1000*(toc - tic)} ms\\n\")  # 10%\n",
        "\n",
        "### VECTORIZED OUTER PRODUCT ###\n",
        "tic = time.process_time()\n",
        "### START CODE HERE ###\n",
        "outer = np.outer(x1, x2, out=None)\n",
        "### END CODE HERE ###\n",
        "toc = time.process_time()\n",
        "print(f\"outer = {outer}\\n----- Computation time = {1000*(toc - tic)} ms\\n\")  # 10%\n",
        "\n",
        "### VECTORIZED ELEMENTWISE MULTIPLICATION ###\n",
        "tic = time.process_time()\n",
        "### START CODE HERE ###\n",
        "mul = np.multiply(x1, x2)\n",
        "### END CODE HERE ###\n",
        "toc = time.process_time()\n",
        "print(f\"elementwise multiplication = {mul}\\n----- Computation time = {1000*(toc - tic)} ms\\n\")  # 10%\n",
        "\n",
        "### VECTORIZED GENERAL DOT PRODUCT ###\n",
        "tic = time.process_time()\n",
        "### START CODE HERE ###\n",
        "W = np.random.rand(3,len(x1))\n",
        "gdot = np.dot(W,x1)\n",
        "### END CODE HERE ###\n",
        "toc = time.process_time()\n",
        "print(f\"gdot = {gdot}\\n----- Computation time = {1000*(toc - tic)} ms\\n\")  # 10%\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3Pq9rOGlwkD"
      },
      "source": [
        "As you may have noticed, the vectorized implementation is much cleaner and more efficient. For bigger vectors/matrices, the differences in running time become even bigger. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXtjVb_blwkD"
      },
      "source": [
        "### 2.2 L1 and L2 loss function \n",
        "\n",
        "#### **(20%) Exercise 2.2: Implement the the L1 and L2 loss functions** \n",
        "Realize L1 and L2 loss function using NumPy vectorizing. \n",
        "\n",
        "**Reminder**:\n",
        "- The loss is used to evaluate the performance of your model. The bigger your loss is, the more different your predictions ($ \\hat{y} $) are from the true values ($y$). In deep learning, you use optimization algorithms like Gradient Descent to train your model and to minimize the cost.\n",
        "- L1 loss is defined as:\n",
        "$$\\begin{align*} & L_1(\\hat{y}, y) = \\sum_{i=0}^m|y^{(i)} - \\hat{y}^{(i)}| \\end{align*}\\tag{6}$$\n",
        "- L2 loss is defined as $$\\begin{align*} & L_2(\\hat{y},y) = \\sum_{i=0}^m(y^{(i)} - \\hat{y}^{(i)})^2 \\end{align*}\\tag{7}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "j5bWU5walwkD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "L1 Loss = 1.1\n",
            "L2 Loss = 0.43\n"
          ]
        }
      ],
      "source": [
        "# GRADED FUNCTION: L1\n",
        "\n",
        "def L1(yhat, y):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    yhat -- vector of size m (predicted labels)\n",
        "    y -- vector of size m (true labels)\n",
        "    \n",
        "    Returns:\n",
        "    loss -- the value of the L1 loss function defined above\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ### (≈ 1 line of code)\n",
        "    loss = np.sum(np.abs(y - yhat))\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return loss\n",
        "\n",
        "def L2(yhat, y):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    yhat -- vector of size m (predicted labels)\n",
        "    y -- vector of size m (true labels)\n",
        "    \n",
        "    Returns:\n",
        "    loss -- the value of the L2 loss function defined above\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ### (≈ 1 line of code)\n",
        "    loss = np.dot(np.abs(y - yhat), np.abs(y - yhat))\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return loss\n",
        "\n",
        "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
        "y = np.array([1, 0, 0, 1, 1])\n",
        "print(f\"L1 Loss = {L1(yhat, y)}\")  # 10%\n",
        "print(f\"L2 Loss = {L2(yhat, y)}\")  # 10%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtUZOpZflwkD"
      },
      "source": [
        ">Expected Output:\n",
        "```python\n",
        "L1 Loss = 1.1\n",
        "L2 Loss = 0.43\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3PUrGLMlwkE"
      },
      "source": [
        "#Congratulations! You have finished this assignment!\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "numpy_basics.ipynb",
      "provenance": []
    },
    "coursera": {
      "course_slug": "neural-networks-deep-learning",
      "graded_item_id": "XHpfv",
      "launcher_item_id": "Zh0CU"
    },
    "kernelspec": {
      "display_name": "Python 3.9.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "d8fe9279b24eb5c6c279942e41b8b8e849b3e5cb2998e15bc84e8c8ade1c1671"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
