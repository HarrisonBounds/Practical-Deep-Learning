{"cells":[{"cell_type":"markdown","metadata":{},"source":["# PyTorch Practice\n","**Due: Mondy, 10/24/2022, 2:15 PM**\n","\n","Welcome to your fifth assignment. You will create and train neural networks using a deep learning framework ([PyTorch](https://pytorch.org/)).\n","\n","Contents:\n","\n","1. (20%) Exercise 1: Load CIFAR10 Dataset\n","2. (10%) Exercise 2: Create Neural Network Model\n","4. (5%) Exerise 3.1: Build DataLoaders\n","5. (5%) Exercise 3.2: Training in One Epoch\n","6. (20%) Exercise 3.3: Training Iterations\n","7. (40%) Exercise 4: Custom Images Test\n","\n","Instructions:\n","- The code between the ### START CODE HERE ### and ### END CODE HERE ### comments will be graded.\n","- **Change variable names at your own risk. Make sure you understand what you are doing.**\n","- Avoid using for-loops and while-loops, unless you are explicitly asked to do so.\n","\n","You will learn:\n","- PyTorch built-in datasets.\n","- Create neural network models using `nn` module.\n","- Train neural network model with handy pre-defined loss functions and optimizers."]},{"cell_type":"markdown","metadata":{"id":"fyO57T9UIcgH"},"source":["\n","## Import Libraries\n","PyTorch has two [primitives to work with data](https://pytorch.org/docs/stable/data.html):\n","``torch.utils.data.DataLoader`` and ``torch.utils.data.Dataset``.\n","``Dataset`` stores the samples and their corresponding labels, and ``DataLoader`` wraps an iterable around\n","the ``Dataset``.\n","\n","**If you installed [PyTorch]((https://pytorch.org/get-started/locally/)) using Conda, you may want to [switch Python interpreter](https://code.visualstudio.com/docs/python/environments) to comply with the Conda environment that hosts PyTorch.**"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"gAyJc1FoIcgJ"},"outputs":[],"source":["import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torchvision import datasets\n","from torchvision.io import read_image\n","from torchvision.transforms import ToTensor, Resize\n","import matplotlib.pyplot as plt\n","import ssl\n","ssl._create_default_https_context = ssl._create_unverified_context\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"VuSHJ43BIcgK"},"source":["## 1 - Load Dateset\n","#### **(20%) Exercise 1**: Load CIFAR10 Dataset\n","Load CIFAR10 dataset from TorchVision datasets. \n","1. Create a training dataset.\n","2. Create a test dataset.\n","3. Inspect the training dataset.\n","4. Visualize samples from the training dataset.\n","\n","> **Hint:** \n","Usually, an image is numerically represented by `(# height pixels, # width pixels, # color channels)`. PyTorch, however, represents an image with a different order `(# color channels, # height pixels, # width pixels)`. You may find `torch.permute()` function useful if trying to show pictures in `Matplotlib`. "]},{"cell_type":"code","execution_count":3,"metadata":{"id":"8fpoJniAIcgL"},"outputs":[{"name":"stdout","output_type":"stream","text":["Files already downloaded and verified\n","number of training examples: 50000\n","training image resolution (CHANNEL, HEIGHT, WIDTH): torch.Size([3, 32, 32])\n","number of test examples: 10000\n","test image resolution (CHANNEL, HEIGHT, WIDTH): torch.Size([3, 32, 32])\n"]},{"ename":"IndexError","evalue":"Dimension out of range (expected to be in range of [-3, 2], but got 3)","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[1;32mc:\\Users\\Owner\\github-classroom\\UCAEngineeringPhysics\\a5-pytorch_practice-HarrisonBounds\\pytorch_practice.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 46>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Owner/github-classroom/UCAEngineeringPhysics/a5-pytorch_practice-HarrisonBounds/pytorch_practice.ipynb#W4sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39m### START CODE HERE ###\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Owner/github-classroom/UCAEngineeringPhysics/a5-pytorch_practice-HarrisonBounds/pytorch_practice.ipynb#W4sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m img, label \u001b[39m=\u001b[39m training_data[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m], training_data[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Owner/github-classroom/UCAEngineeringPhysics/a5-pytorch_practice-HarrisonBounds/pytorch_practice.ipynb#W4sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m img_perm \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mpermute(img, (rows, cols, i))  \u001b[39m# you may want to permute order of the axes in img\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Owner/github-classroom/UCAEngineeringPhysics/a5-pytorch_practice-HarrisonBounds/pytorch_practice.ipynb#W4sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39m### END CODE HERE ###\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Owner/github-classroom/UCAEngineeringPhysics/a5-pytorch_practice-HarrisonBounds/pytorch_practice.ipynb#W4sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m figure\u001b[39m.\u001b[39madd_subplot(rows, cols, i)\n","\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-3, 2], but got 3)"]},{"data":{"text/plain":["<Figure size 800x800 with 0 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# Create datasets\n","### START CODE HERE ###\n","training_data = datasets.CIFAR10(\n","        root=\"data\", \n","        train=True, \n","        transform=ToTensor(),\n","        download=True, \n","    )  # Create training data from open datasets.\n","test_data = datasets.CIFAR10(\n","        root=\"data\",\n","        train=False,\n","        transform=ToTensor(),\n","        download=False,\n","      )  # Create test data from open datasets.\n","### END CODE HERE ###\n","\n","# Investigate datasets\n","### START CODE HERE ###\n","num_train = len(training_data)  # number of training examples\n","res_train = training_data[0][0].size()  # training image resolution\n","num_test = len(test_data)  # number of test examples   \n","res_test = test_data[0][0].size() # test image resolution\n","### END CODE HERE ###\n","\n","print(f\"number of training examples: {num_train}\")\n","print(f\"training image resolution (CHANNEL, HEIGHT, WIDTH): {res_train}\")\n","print(f\"number of test examples: {num_test}\")\n","print(f\"test image resolution (CHANNEL, HEIGHT, WIDTH): {res_test}\")\n","\n","\n","# Visulization\n","labels_map = {\n","    0: 'airplane',\n","    1: 'automobile',\n","    2: 'bird',\n","    3: 'cat',\n","    4: 'deer',\n","    5: 'dog',\n","    6: 'frog',\n","    7: 'horse',\n","    8: 'ship',\n","    9: 'truck',\n","}\n","figure = plt.figure(figsize=(8, 8))\n","cols, rows = 3, 3\n","for i in range(1, cols * rows + 1):\n","    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n","    ### START CODE HERE ###\n","    img, label = training_data[sample_idx][0], training_data[sample_idx][1]\n","    img_perm = torch.permute(img, (1, 2, 0))  # you may want to permute order of the axes in img\n","    ### END CODE HERE ###\n","    figure.add_subplot(rows, cols, i)\n","    plt.title(labels_map[label])\n","    plt.axis(\"off\")\n","    plt.imshow(img_perm.squeeze(), cmap=\"gray\")\n"]},{"cell_type":"markdown","metadata":{},"source":["> **Expected Results:**\n","```console\n","number of training examples: 50000\n","training image resolution (CHANNEL, HEIGHT, WIDTH): torch.Size([3, 32, 32])\n","number of test examples: 10000\n","test image resolution (CHANNEL, HEIGHT, WIDTH): torch.Size([3, 32, 32])\n","```\n"]},{"cell_type":"markdown","metadata":{},"source":["--------------\n"]},{"cell_type":"markdown","metadata":{"id":"w9ZKhUImIcgR"},"source":["## 2 - Build the Model\n","#### **(10%) Exercise 2**: Create Neural Network Model\n","Define a neural network model class from [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). The NN model contains 3 layers (2 hidden layers and 1 output layer).\n","- Define the layers of the model in `__init__` function (define a helper function: `flatten`).\n","- Define how data will pass through the model in the `forward` function (use `flatten` function to reshape the inputs). \n","- (Optional) Move the operations to GPU if available.\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"C95DaIoSIcgS","outputId":"8ea12c8d-3608-4cec-827e-d449fde84302"},"outputs":[{"name":"stdout","output_type":"stream","text":["NeuralNetwork(\n","  (flatten): Flatten(start_dim=1, end_dim=-1)\n","  (linear_relu_stack): Sequential(\n","    (0): Linear(in_features=3072, out_features=100, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=100, out_features=200, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=200, out_features=10, bias=True)\n","  )\n",")\n","Predicted probabilities: \n","tensor([[0.0906, 0.1013, 0.1041, 0.1044, 0.0954, 0.0921, 0.1114, 0.0877, 0.1031,\n","         0.1099]], grad_fn=<SoftmaxBackward0>)\n","Predicted classes: tensor([6]): frog\n"]}],"source":["class NeuralNetwork(nn.Module):\n","\n","    def __init__(self, hidden_layer_sizes):\n","        \"\"\"\n","        Structure of the neural network model\n","\n","        Arguments:\n","            hiddent_layer_sizes -- python list, contains two values to indicate number of neurons in the first and second hiddent layer\n","        \"\"\"\n","\n","        super().__init__()\n","        self.flatten = nn.Flatten()\n","        ### START CODE HERE ###\n","        self.linear_relu_stack = nn.Sequential(\n","            nn.Linear(32 * 32 * 3, 100),\n","            nn.ReLU(),\n","            nn.Linear(10 * 10, 200),\n","            nn.ReLU(),\n","            nn.Linear(10 * 20, 10))\n","        ### END CODE HERE ###\n","    \n","    def forward(self, inputs):\n","        ### START CODE HERE ###\n","        x = self.flatten(inputs)\n","        logits = self.linear_relu_stack(x)\n","        ### END CODE HERE ###\n","    \n","        return logits\n","\n","\n","# Test/Usage\n","model = NeuralNetwork([100, 200])\n","print(model)\n","\n","X = torch.rand(1, 3, 32, 32)\n","logits = model(X)  # DO NOT call model.forward()\n","pred_probab = nn.Softmax(dim=1)(logits)\n","y_pred = pred_probab.argmax(1)\n","print(f\"Predicted probabilities: \\n{pred_probab}\")\n","print(f\"Predicted classes: {y_pred}: {labels_map[int(y_pred)]}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["> **Expected:**\n","```console\n","NeuralNetwork(\n","  (flatten): Flatten(start_dim=1, end_dim=-1)\n","  (linear_relu_stack): Sequential(\n","    (0): Linear(in_features=3072, out_features=100, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=100, out_features=200, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=200, out_features=10, bias=True)\n","  )\n",")\n","Predicted probabilities: \n","tensor([[0.0893, 0.0994, 0.0983, 0.0990, 0.0937, 0.0988, 0.1027, 0.1051, 0.1146,\n","         0.0991]], grad_fn=<SoftmaxBackward0>)\n","Predicted classes: tensor([8]): ship\n","```"]},{"cell_type":"markdown","metadata":{"id":"B5aW2QWlIcgS"},"source":["> To use the model, we pass it the input data. This executes the modelâ€™s `forward`, along with some [background operations](https://github.com/pytorch/pytorch/blob/270111b7b611d174967ed204776985cefca9c144/torch/nn/modules/module.py#L866). Do not call `model.forward()` directly!\n","\n","> Calling the model on the input returns a 2-dimensional tensor with dim=0 corresponding to each output of 10 raw predicted values for each class, and dim=1 corresponding to the individual values of each output. We get the prediction probabilities by passing it through an instance of the `nn.Softmax` module.\n","\n","> Read more about [building neural networks in PyTorch](https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html).\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"w2LELznUIcgU"},"source":["--------------\n"]},{"cell_type":"markdown","metadata":{"id":"QTVRPcczIcgU"},"source":["## 3- Optimize the Model Parameters\n","\n","#### **(5%) Exercise 3.1**: Build DataLoaders\n","Pass the `Dataset` as an argument to `DataLoader`. This wraps an iterable over the dataset, and supports\n","automatic batching, sampling, shuffling and multiprocess data loading. \n","- Create a `DataLoader` for `training_data` with default batch size of 64.\n","- Create a `DataLoader` for `test_data` with default batch size of 64.\n","\n","> **Update the `batch_size` later if needed**"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of X in batch 1 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 1: torch.Size([64]) torch.int64\n","Shape of X in batch 2 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 2: torch.Size([64]) torch.int64\n","Shape of X in batch 3 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 3: torch.Size([64]) torch.int64\n","Shape of X in batch 4 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 4: torch.Size([64]) torch.int64\n","Shape of X in batch 5 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 5: torch.Size([64]) torch.int64\n","Shape of X in batch 6 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 6: torch.Size([64]) torch.int64\n","Shape of X in batch 7 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 7: torch.Size([64]) torch.int64\n","Shape of X in batch 8 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 8: torch.Size([64]) torch.int64\n","Shape of X in batch 9 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 9: torch.Size([64]) torch.int64\n","Shape of X in batch 10 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 10: torch.Size([64]) torch.int64\n","Shape of X in batch 11 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 11: torch.Size([64]) torch.int64\n","Shape of X in batch 12 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 12: torch.Size([64]) torch.int64\n","Shape of X in batch 13 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 13: torch.Size([64]) torch.int64\n","Shape of X in batch 14 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 14: torch.Size([64]) torch.int64\n","Shape of X in batch 15 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 15: torch.Size([64]) torch.int64\n","Shape of X in batch 16 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 16: torch.Size([64]) torch.int64\n","Shape of X in batch 17 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 17: torch.Size([64]) torch.int64\n","Shape of X in batch 18 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 18: torch.Size([64]) torch.int64\n","Shape of X in batch 19 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 19: torch.Size([64]) torch.int64\n","Shape of X in batch 20 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 20: torch.Size([64]) torch.int64\n","Shape of X in batch 21 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 21: torch.Size([64]) torch.int64\n","Shape of X in batch 22 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 22: torch.Size([64]) torch.int64\n","Shape of X in batch 23 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 23: torch.Size([64]) torch.int64\n","Shape of X in batch 24 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 24: torch.Size([64]) torch.int64\n","Shape of X in batch 25 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 25: torch.Size([64]) torch.int64\n","Shape of X in batch 26 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 26: torch.Size([64]) torch.int64\n","Shape of X in batch 27 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 27: torch.Size([64]) torch.int64\n","Shape of X in batch 28 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 28: torch.Size([64]) torch.int64\n","Shape of X in batch 29 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 29: torch.Size([64]) torch.int64\n","Shape of X in batch 30 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 30: torch.Size([64]) torch.int64\n","Shape of X in batch 31 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 31: torch.Size([64]) torch.int64\n","Shape of X in batch 32 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 32: torch.Size([64]) torch.int64\n","Shape of X in batch 33 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 33: torch.Size([64]) torch.int64\n","Shape of X in batch 34 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 34: torch.Size([64]) torch.int64\n","Shape of X in batch 35 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 35: torch.Size([64]) torch.int64\n","Shape of X in batch 36 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 36: torch.Size([64]) torch.int64\n","Shape of X in batch 37 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 37: torch.Size([64]) torch.int64\n","Shape of X in batch 38 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 38: torch.Size([64]) torch.int64\n","Shape of X in batch 39 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 39: torch.Size([64]) torch.int64\n","Shape of X in batch 40 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 40: torch.Size([64]) torch.int64\n","Shape of X in batch 41 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 41: torch.Size([64]) torch.int64\n","Shape of X in batch 42 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 42: torch.Size([64]) torch.int64\n","Shape of X in batch 43 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 43: torch.Size([64]) torch.int64\n","Shape of X in batch 44 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 44: torch.Size([64]) torch.int64\n","Shape of X in batch 45 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 45: torch.Size([64]) torch.int64\n","Shape of X in batch 46 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 46: torch.Size([64]) torch.int64\n","Shape of X in batch 47 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 47: torch.Size([64]) torch.int64\n","Shape of X in batch 48 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 48: torch.Size([64]) torch.int64\n","Shape of X in batch 49 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 49: torch.Size([64]) torch.int64\n","Shape of X in batch 50 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 50: torch.Size([64]) torch.int64\n","Shape of X in batch 51 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 51: torch.Size([64]) torch.int64\n","Shape of X in batch 52 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 52: torch.Size([64]) torch.int64\n","Shape of X in batch 53 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 53: torch.Size([64]) torch.int64\n","Shape of X in batch 54 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 54: torch.Size([64]) torch.int64\n","Shape of X in batch 55 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 55: torch.Size([64]) torch.int64\n","Shape of X in batch 56 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 56: torch.Size([64]) torch.int64\n","Shape of X in batch 57 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 57: torch.Size([64]) torch.int64\n","Shape of X in batch 58 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 58: torch.Size([64]) torch.int64\n","Shape of X in batch 59 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 59: torch.Size([64]) torch.int64\n","Shape of X in batch 60 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 60: torch.Size([64]) torch.int64\n","Shape of X in batch 61 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 61: torch.Size([64]) torch.int64\n","Shape of X in batch 62 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 62: torch.Size([64]) torch.int64\n","Shape of X in batch 63 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 63: torch.Size([64]) torch.int64\n","Shape of X in batch 64 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 64: torch.Size([64]) torch.int64\n","Shape of X in batch 65 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 65: torch.Size([64]) torch.int64\n","Shape of X in batch 66 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 66: torch.Size([64]) torch.int64\n","Shape of X in batch 67 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 67: torch.Size([64]) torch.int64\n","Shape of X in batch 68 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 68: torch.Size([64]) torch.int64\n","Shape of X in batch 69 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 69: torch.Size([64]) torch.int64\n","Shape of X in batch 70 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 70: torch.Size([64]) torch.int64\n","Shape of X in batch 71 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 71: torch.Size([64]) torch.int64\n","Shape of X in batch 72 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 72: torch.Size([64]) torch.int64\n","Shape of X in batch 73 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 73: torch.Size([64]) torch.int64\n","Shape of X in batch 74 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 74: torch.Size([64]) torch.int64\n","Shape of X in batch 75 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 75: torch.Size([64]) torch.int64\n","Shape of X in batch 76 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 76: torch.Size([64]) torch.int64\n","Shape of X in batch 77 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 77: torch.Size([64]) torch.int64\n","Shape of X in batch 78 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 78: torch.Size([64]) torch.int64\n","Shape of X in batch 79 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 79: torch.Size([64]) torch.int64\n","Shape of X in batch 80 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 80: torch.Size([64]) torch.int64\n","Shape of X in batch 81 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 81: torch.Size([64]) torch.int64\n","Shape of X in batch 82 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 82: torch.Size([64]) torch.int64\n","Shape of X in batch 83 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 83: torch.Size([64]) torch.int64\n","Shape of X in batch 84 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 84: torch.Size([64]) torch.int64\n","Shape of X in batch 85 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 85: torch.Size([64]) torch.int64\n","Shape of X in batch 86 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 86: torch.Size([64]) torch.int64\n","Shape of X in batch 87 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 87: torch.Size([64]) torch.int64\n","Shape of X in batch 88 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 88: torch.Size([64]) torch.int64\n","Shape of X in batch 89 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 89: torch.Size([64]) torch.int64\n","Shape of X in batch 90 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 90: torch.Size([64]) torch.int64\n","Shape of X in batch 91 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 91: torch.Size([64]) torch.int64\n","Shape of X in batch 92 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 92: torch.Size([64]) torch.int64\n","Shape of X in batch 93 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 93: torch.Size([64]) torch.int64\n","Shape of X in batch 94 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 94: torch.Size([64]) torch.int64\n","Shape of X in batch 95 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 95: torch.Size([64]) torch.int64\n","Shape of X in batch 96 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 96: torch.Size([64]) torch.int64\n","Shape of X in batch 97 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 97: torch.Size([64]) torch.int64\n","Shape of X in batch 98 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 98: torch.Size([64]) torch.int64\n","Shape of X in batch 99 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 99: torch.Size([64]) torch.int64\n","Shape of X in batch 100 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 100: torch.Size([64]) torch.int64\n","Shape of X in batch 101 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 101: torch.Size([64]) torch.int64\n","Shape of X in batch 102 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 102: torch.Size([64]) torch.int64\n","Shape of X in batch 103 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 103: torch.Size([64]) torch.int64\n","Shape of X in batch 104 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 104: torch.Size([64]) torch.int64\n","Shape of X in batch 105 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 105: torch.Size([64]) torch.int64\n","Shape of X in batch 106 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 106: torch.Size([64]) torch.int64\n","Shape of X in batch 107 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 107: torch.Size([64]) torch.int64\n","Shape of X in batch 108 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 108: torch.Size([64]) torch.int64\n","Shape of X in batch 109 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 109: torch.Size([64]) torch.int64\n","Shape of X in batch 110 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 110: torch.Size([64]) torch.int64\n","Shape of X in batch 111 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 111: torch.Size([64]) torch.int64\n","Shape of X in batch 112 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 112: torch.Size([64]) torch.int64\n","Shape of X in batch 113 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 113: torch.Size([64]) torch.int64\n","Shape of X in batch 114 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 114: torch.Size([64]) torch.int64\n","Shape of X in batch 115 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 115: torch.Size([64]) torch.int64\n","Shape of X in batch 116 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 116: torch.Size([64]) torch.int64\n","Shape of X in batch 117 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 117: torch.Size([64]) torch.int64\n","Shape of X in batch 118 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 118: torch.Size([64]) torch.int64\n","Shape of X in batch 119 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 119: torch.Size([64]) torch.int64\n","Shape of X in batch 120 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 120: torch.Size([64]) torch.int64\n","Shape of X in batch 121 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 121: torch.Size([64]) torch.int64\n","Shape of X in batch 122 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 122: torch.Size([64]) torch.int64\n","Shape of X in batch 123 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 123: torch.Size([64]) torch.int64\n","Shape of X in batch 124 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 124: torch.Size([64]) torch.int64\n","Shape of X in batch 125 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 125: torch.Size([64]) torch.int64\n","Shape of X in batch 126 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 126: torch.Size([64]) torch.int64\n","Shape of X in batch 127 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 127: torch.Size([64]) torch.int64\n","Shape of X in batch 128 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 128: torch.Size([64]) torch.int64\n","Shape of X in batch 129 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 129: torch.Size([64]) torch.int64\n","Shape of X in batch 130 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 130: torch.Size([64]) torch.int64\n","Shape of X in batch 131 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 131: torch.Size([64]) torch.int64\n","Shape of X in batch 132 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 132: torch.Size([64]) torch.int64\n","Shape of X in batch 133 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 133: torch.Size([64]) torch.int64\n","Shape of X in batch 134 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 134: torch.Size([64]) torch.int64\n","Shape of X in batch 135 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 135: torch.Size([64]) torch.int64\n","Shape of X in batch 136 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 136: torch.Size([64]) torch.int64\n","Shape of X in batch 137 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 137: torch.Size([64]) torch.int64\n","Shape of X in batch 138 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 138: torch.Size([64]) torch.int64\n","Shape of X in batch 139 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 139: torch.Size([64]) torch.int64\n","Shape of X in batch 140 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 140: torch.Size([64]) torch.int64\n","Shape of X in batch 141 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 141: torch.Size([64]) torch.int64\n","Shape of X in batch 142 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 142: torch.Size([64]) torch.int64\n","Shape of X in batch 143 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 143: torch.Size([64]) torch.int64\n","Shape of X in batch 144 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 144: torch.Size([64]) torch.int64\n","Shape of X in batch 145 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 145: torch.Size([64]) torch.int64\n","Shape of X in batch 146 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 146: torch.Size([64]) torch.int64\n","Shape of X in batch 147 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 147: torch.Size([64]) torch.int64\n","Shape of X in batch 148 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 148: torch.Size([64]) torch.int64\n","Shape of X in batch 149 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 149: torch.Size([64]) torch.int64\n","Shape of X in batch 150 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 150: torch.Size([64]) torch.int64\n","Shape of X in batch 151 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 151: torch.Size([64]) torch.int64\n","Shape of X in batch 152 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 152: torch.Size([64]) torch.int64\n","Shape of X in batch 153 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 153: torch.Size([64]) torch.int64\n","Shape of X in batch 154 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 154: torch.Size([64]) torch.int64\n","Shape of X in batch 155 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 155: torch.Size([64]) torch.int64\n","Shape of X in batch 156 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 156: torch.Size([64]) torch.int64\n","Shape of X in batch 157 [N, C, H, W]: torch.Size([16, 3, 32, 32])\n","Shape of y in batch 157: torch.Size([16]) torch.int64\n"]}],"source":["### START CODE HERE ###\n","train_dataloader = DataLoader(training_data, batch_size=64)\n","test_dataloader = DataLoader(test_data, batch_size=64)\n","### END CODE HERE ###\n","\n","\n","# Test\n","i = 1\n","for X, y in test_dataloader:\n","    print(f\"Shape of X in batch {i} [N, C, H, W]: {X.shape}\")\n","    print(f\"Shape of y in batch {i}: {y.shape} {y.dtype}\")\n","    i += 1\n","    # break\n"]},{"cell_type":"markdown","metadata":{},"source":["> **Expected:**\n","```console\n","Shape of X in batch 1 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 1: torch.Size([64]) torch.int64\n","Shape of X in batch 2 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 2: torch.Size([64]) torch.int64\n","...\n","Shape of X in batch 156 [N, C, H, W]: torch.Size([64, 3, 32, 32])\n","Shape of y in batch 156: torch.Size([64]) torch.int64\n","Shape of X in batch 157 [N, C, H, W]: torch.Size([16, 3, 32, 32])\n","Shape of y in batch 157: torch.Size([16]) torch.int64\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"Toyg9mN3IcgV"},"source":["#### **(5%) Exercise 3.2**: Training in One Epoch\n","Define the `train` function to loop all the batches in the DataLoader. In a single training loop, \n","1. the model makes predictions on the training minibatches: `model(inputs)`, \n","2. compute prediction error: `loss_fn(predictions, labels)`,\n","3. backpropagate to obtain the gradients of the model's parameters: `loss.backward()`,\n","4. update the model's parameters with the specified optimizer: `optimizer.step()`,\n"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"6kLHCs0EIcgV"},"outputs":[],"source":["def train(dataloader, model, loss_fn, optimizer):\n","    size = len(dataloader.dataset)\n","    model.train()\n","    for batch, (X, y) in enumerate(dataloader):\n","        # Compute prediction error\n","        ### START CODE HERE ###\n","        pred = model(X)  # forward propagation\n","        loss = loss_fn(pred, y)  # compute loss\n","        optimizer.zero_grad()  # zero previous gradient\n","        loss.backward()  # back propagatin\n","        optimizer.step()  # update parameters\n","        ### END CODE HERE ###\n","        if batch % 100 == 0:\n","            loss, current = loss.item(), batch * len(X)\n","            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n","\n","\n","# Define a test function to evaluate model performance\n","def test(dataloader, model, loss_fn):\n","    size = len(dataloader.dataset)\n","    num_batches = len(dataloader)\n","    model.eval()\n","    test_loss, accuracy = 0, 0\n","    with torch.no_grad():\n","        for X, y in dataloader:\n","            # X, y = X.to(device), y.to(device)\n","            pred = model(X)\n","            test_loss += loss_fn(pred, y).item()\n","            accuracy += (pred.argmax(1) == y).type(torch.float).sum().item()\n","    test_loss /= num_batches\n","    accuracy /= size\n","    print(f\"Test Error: \\n Accuracy: {(100*accuracy):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n","\n","    return accuracy\n"]},{"cell_type":"markdown","metadata":{},"source":["#### **(20%) Exercise 3.3**: Training Iterations\n","The training process is conducted over several iterations (*epochs*). During each epoch, the model learns\n","parameters to make better predictions. Log the model's accuracy and loss at each epoch; we'd like to see the\n","accuracy increase and the loss decrease with every epoch.\n","\n","To start a training process:\n","1. initialize neural network model with your choice of hidden layer sizes,\n","2. define cross-entropy loss function,\n","3. specify Adam optimizer with your choice of learning rate to update the model's parameters,\n","4. set total number of training epochs.\n","\n","You are expected to obtain a growing accuracy curve. \n","> **Hint:**\n","> - Feel free to change whatever hyperparameters to increase the test accuracy\n","> - You can choose different `batch_size`"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1\n","-------------------------------\n","loss: 2.304517  [    0/50000]\n","loss: 2.305483  [ 6400/50000]\n","loss: 2.286084  [12800/50000]\n","loss: 2.305195  [19200/50000]\n","loss: 2.288111  [25600/50000]\n","loss: 2.281210  [32000/50000]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32mc:\\Users\\Owner\\github-classroom\\UCAEngineeringPhysics\\a5-pytorch_practice-HarrisonBounds\\pytorch_practice.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Owner/github-classroom/UCAEngineeringPhysics/a5-pytorch_practice-HarrisonBounds/pytorch_practice.ipynb#X24sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Owner/github-classroom/UCAEngineeringPhysics/a5-pytorch_practice-HarrisonBounds/pytorch_practice.ipynb#X24sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m-------------------------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Owner/github-classroom/UCAEngineeringPhysics/a5-pytorch_practice-HarrisonBounds/pytorch_practice.ipynb#X24sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     train(train_dataloader, cifar_model, loss_fn, optimizer)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Owner/github-classroom/UCAEngineeringPhysics/a5-pytorch_practice-HarrisonBounds/pytorch_practice.ipynb#X24sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     acc \u001b[39m=\u001b[39m test(test_dataloader, cifar_model, loss_fn)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Owner/github-classroom/UCAEngineeringPhysics/a5-pytorch_practice-HarrisonBounds/pytorch_practice.ipynb#X24sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     test_acc\u001b[39m.\u001b[39mappend(acc)\n","\u001b[1;32mc:\\Users\\Owner\\github-classroom\\UCAEngineeringPhysics\\a5-pytorch_practice-HarrisonBounds\\pytorch_practice.ipynb Cell 19\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Owner/github-classroom/UCAEngineeringPhysics/a5-pytorch_practice-HarrisonBounds/pytorch_practice.ipynb#X24sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(dataloader\u001b[39m.\u001b[39mdataset)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Owner/github-classroom/UCAEngineeringPhysics/a5-pytorch_practice-HarrisonBounds/pytorch_practice.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Owner/github-classroom/UCAEngineeringPhysics/a5-pytorch_practice-HarrisonBounds/pytorch_practice.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch, (X, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Owner/github-classroom/UCAEngineeringPhysics/a5-pytorch_practice-HarrisonBounds/pytorch_practice.ipynb#X24sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m# Compute prediction error\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Owner/github-classroom/UCAEngineeringPhysics/a5-pytorch_practice-HarrisonBounds/pytorch_practice.ipynb#X24sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m### START CODE HERE ###\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Owner/github-classroom/UCAEngineeringPhysics/a5-pytorch_practice-HarrisonBounds/pytorch_practice.ipynb#X24sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     pred \u001b[39m=\u001b[39m model(X)  \u001b[39m# forward propagation\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Owner/github-classroom/UCAEngineeringPhysics/a5-pytorch_practice-HarrisonBounds/pytorch_practice.ipynb#X24sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     loss \u001b[39m=\u001b[39m loss_fn(pred, y)  \u001b[39m# compute loss\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[1;32mc:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n","File \u001b[1;32mc:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[1;32mc:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[1;32mc:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\datasets\\cifar.py:118\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    115\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img)\n\u001b[0;32m    117\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 118\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[0;32m    120\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    121\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n","File \u001b[1;32mc:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\transforms\\transforms.py:134\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[0;32m    127\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(pic)\n","File \u001b[1;32mc:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\transforms\\functional.py:168\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[39mif\u001b[39;00m pic\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    167\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39m255\u001b[39m \u001b[39m*\u001b[39m img\n\u001b[1;32m--> 168\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39;49mview(pic\u001b[39m.\u001b[39;49msize[\u001b[39m1\u001b[39;49m], pic\u001b[39m.\u001b[39;49msize[\u001b[39m0\u001b[39;49m], \u001b[39mlen\u001b[39;49m(pic\u001b[39m.\u001b[39;49mgetbands()))\n\u001b[0;32m    169\u001b[0m \u001b[39m# put it from HWC to CHW format\u001b[39;00m\n\u001b[0;32m    170\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mpermute((\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mcontiguous()\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Start training\n","### START CODE HERE ###\n","cifar_model = NeuralNetwork(100)\n","loss_fn = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(cifar_model.parameters(), lr=1e-3)\n","epochs = 4\n","### END CODE HERE ###\n","test_acc = []\n","for t in range(epochs):\n","    print(f\"Epoch {t+1}\\n-------------------------------\")\n","    train(train_dataloader, cifar_model, loss_fn, optimizer)\n","    acc = test(test_dataloader, cifar_model, loss_fn)\n","    test_acc.append(acc)\n","print(\"Done!\")\n","\n","\n","plt.plot(test_acc)\n"]},{"cell_type":"markdown","metadata":{"id":"qY-z6wgJIcgY"},"source":["--------------\n"]},{"cell_type":"markdown","metadata":{"id":"87yQW0tAIcgZ"},"source":["## 4 - Evaluation\n","This model can now be used to make predictions. You can change the index of the test example from `test_data` in the following block. \n"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"18koQ2NcIcga","outputId":"5a26d21b-67d2-47cc-b358-c394187996e5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Predicted: \"ship\", Actual: \"frog\"\n"]},{"data":{"text/plain":["<matplotlib.image.AxesImage at 0x18093a538b0>"]},"execution_count":21,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["cifar_model.eval()\n","x, y = test_data[11][0], test_data[4][1]\n","x = x.unsqueeze(dim=0)\n","with torch.no_grad():\n","    pred = cifar_model(x)\n","    predicted, actual = labels_map[int(pred[0].argmax(0))], labels_map[y]\n","    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')\n","plt.imshow(x.squeeze().permute(1, 2, 0), cmap=\"gray\")\n"]},{"cell_type":"markdown","metadata":{},"source":["#### **(40%) Exercise 4**: Test Custom Images\n","Upload 10 custom pictures to the `images/` directory, one picture for each class. Evaluate and observe the predicted results with the trained model and newly uploaded pictures.   \n","\n","> **Hints:**\n","> - To read a image as a torch tensor, check out [`torchvision.io.read_image()`](https://pytorch.org/vision/stable/generated/torchvision.io.read_image.html#torchvision.io.read_image) function.\n","> - To resize a torch tensor (image), check out ['torchvision.transforms.Resize'](https://pytorch.org/vision/main/generated/torchvision.transforms.Resize.html) class or [this tutorial](https://www.tutorialspoint.com/pytorch-how-to-resize-an-image-to-a-given-size).\n","> - Images named after `class name.jpg` pattern would be easier to use. For example: `frog.jpg` and `ship.jpg`.\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"ename":"RuntimeError","evalue":"mat1 and mat2 shapes cannot be multiplied (3x1024 and 3072x100)","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[1;32mc:\\Users\\Owner\\github-classroom\\UCAEngineeringPhysics\\a5-pytorch_practice-HarrisonBounds\\pytorch_practice.ipynb Cell 24\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Owner/github-classroom/UCAEngineeringPhysics/a5-pytorch_practice-HarrisonBounds/pytorch_practice.ipynb#X32sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m img \u001b[39m=\u001b[39m test_data[\u001b[39m20\u001b[39m][\u001b[39m0\u001b[39m]  \u001b[39m# read image to tensor\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Owner/github-classroom/UCAEngineeringPhysics/a5-pytorch_practice-HarrisonBounds/pytorch_practice.ipynb#X32sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m img_resize \u001b[39m=\u001b[39m resize(img) \u001b[39m# resize image to fit in the nn model\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Owner/github-classroom/UCAEngineeringPhysics/a5-pytorch_practice-HarrisonBounds/pytorch_practice.ipynb#X32sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m pred \u001b[39m=\u001b[39m cifar_model(img_resize)  \u001b[39m# make prediction\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Owner/github-classroom/UCAEngineeringPhysics/a5-pytorch_practice-HarrisonBounds/pytorch_practice.ipynb#X32sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m img_perm \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mpermute(img, (\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m))  \u001b[39m# permute image axes to compatible with matplotlib\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Owner/github-classroom/UCAEngineeringPhysics/a5-pytorch_practice-HarrisonBounds/pytorch_practice.ipynb#X32sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m### END CODE HERE ###\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","\u001b[1;32mc:\\Users\\Owner\\github-classroom\\UCAEngineeringPhysics\\a5-pytorch_practice-HarrisonBounds\\pytorch_practice.ipynb Cell 24\u001b[0m in \u001b[0;36mNeuralNetwork.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Owner/github-classroom/UCAEngineeringPhysics/a5-pytorch_practice-HarrisonBounds/pytorch_practice.ipynb#X32sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, inputs):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Owner/github-classroom/UCAEngineeringPhysics/a5-pytorch_practice-HarrisonBounds/pytorch_practice.ipynb#X32sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39m### START CODE HERE ###\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Owner/github-classroom/UCAEngineeringPhysics/a5-pytorch_practice-HarrisonBounds/pytorch_practice.ipynb#X32sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflatten(inputs)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Owner/github-classroom/UCAEngineeringPhysics/a5-pytorch_practice-HarrisonBounds/pytorch_practice.ipynb#X32sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear_relu_stack(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Owner/github-classroom/UCAEngineeringPhysics/a5-pytorch_practice-HarrisonBounds/pytorch_practice.ipynb#X32sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     \u001b[39m### END CODE HERE ###\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Owner/github-classroom/UCAEngineeringPhysics/a5-pytorch_practice-HarrisonBounds/pytorch_practice.ipynb#X32sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m logits\n","File \u001b[1;32mc:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n","File \u001b[1;32mc:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n","\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (3x1024 and 3072x100)"]},{"data":{"text/plain":["<Figure size 640x480 with 0 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["\n","figure = plt.figure()\n","cols, rows = 2, 5\n","resize = Resize((32, 32))\n","to_tensor = ToTensor()\n","img_tensor = to_tensor(img)\n","for i in range(1, cols * rows + 1):\n","    ### START CODE HERE ###\n","    img = test_data[20][0]  # read image to tensor\n","    img_resize = resize(img_tensor) # resize image to fit in the nn model\n","    pred = cifar_model(img_resize)  # make prediction\n","    img_perm = torch.permute(img, (1, 2, 0))  # permute image axes to compatible with matplotlib\n","    ### END CODE HERE ###\n","    figure.add_subplot(rows, cols, i)\n","    plt.title('label:' + labels_map[i-1] + '\\tprediction:' + labels_map[pred])\n","    plt.axis(\"off\")\n","    plt.imshow(img_perm.squeeze(), cmap=\"gray\")\n"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3.9.5 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"},"vscode":{"interpreter":{"hash":"d8fe9279b24eb5c6c279942e41b8b8e849b3e5cb2998e15bc84e8c8ade1c1671"}}},"nbformat":4,"nbformat_minor":0}
